# Dark siren GR-truth calibration (catalog injection suite)

Date: **2026-02-06**  
Primary artifact bundle: `final_final_final/` (this folder)  
Source run directory: `outputs/dark_siren_catalog_injection_gr_n512_draw256_hb60_fast_20260206_100811UTC/`

## What this is

This is a **known-truth calibration** of the GWTC-3 dark-siren “gap test” scoring pipeline.

The motivating observation (real data) is a positive propagation preference:
- Real-data baseline score: `ΔLPD_total ≈ +3.03` (Bayes proxy `≈ e^3 ≈ 20`) in  
  `outputs/dark_siren_gap_pe_scaleup36max_20260201_155611UTC/summary_M0_start101.json`.
- Rotation-null controls (sky randomization) show a similar score, indicating the real-data preference is largely **sky-independent** and therefore not driven by unique host associations (see README for the exact paths/results).

Given that mechanism, the most important next question is:

> If the truth is GR propagation, does our full catalog+selection+incompleteness scoring pipeline ever “hallucinate” a large positive `ΔLPD_total` anyway?

This report answers that question with a **catalog-based injection suite** under a GR truth.

## Definitions (as used in code)

- `LPD(model)`: log predictive density under the model (log posterior predictive).
- `ΔLPD_total = LPD(μ-propagation) − LPD(GR-baseline)`.
  - Positive: μ-propagation predicts the data better than GR baseline.
  - Negative: GR baseline predicts the data better.
- Score decomposition:
  - `ΔLPD_total_data`: contribution from the **event likelihoods** (catalog + missing-host mixture), without the selection normalization.
  - `ΔLPD_total_sel`: contribution from the **selection normalization** (α) term.
  - By construction: `ΔLPD_total = ΔLPD_total_data + ΔLPD_total_sel`.

## The test we ran (Tier 3 synthetic injection; GR truth)

We ran `scripts/run_dark_siren_catalog_injection_suite.py` with `truth_model=gr`.

High-level procedure per replicate (see `scripts/run_dark_siren_catalog_injection_suite.py`):
1. Pick one posterior draw `j_truth` from the reconstructed background/μ posterior.
2. For each of the 36 GWTC-3 template events:
   - Sample a “true” redshift `z_true` from that event’s cached redshift histogram.
   - Compute the EM luminosity distance `dL_em_true(z_true)` from the `H(z)` draw.
   - Set GW propagation ratio `R_true(z)=1` (GR truth), hence `dL_true=dL_em_true`.
   - Build a synthetic, sky-independent PE distance histogram centered on `dL_true` with a lognormal-like likelihood width inferred from the template.
3. Score that synthetic dataset under:
   - the **μ-propagation** model implied by the same reconstructed posterior draws, and
   - the **GR baseline** (`R(z)=1`),
   including the same incompleteness mixture treatment and the selection normalization α term.
4. Record `ΔLPD_total`, plus `data` vs `sel` components.

This is a parametric-bootstrap-style calibration: it answers “what does our pipeline output when the world is generated by the same model family, with GR propagation?”

## Run configuration (exact)

The launch script is preserved verbatim:
- `final_final_final/artifacts/job.sh`

Key parameters:
- `truth_model`: `gr`
- `n_rep`: `512`
- `n_events`: `36`
- `n_draws`: `256`
- `n_proc`: `256`
- `heartbeat_sec`: `60`
- `partial_write_min_sec`: `10`

Full config snapshot:
- `final_final_final/artifacts/manifest.json`

Selection + incompleteness settings of note (from `manifest.json`):
- Selection correction enabled (O3 injections; IFAR ≥ 1 yr; binned-SNR detection model).
- Incompleteness mixture `f_miss` marginalized on a fixed grid.
- Missing-host prior: comoving-uniform `p(z)` on `[0, 0.3]`.

## Monitorability / “is it stuck?” guarantees

This run is designed to be monitorable even if a single replicate is slow:
- A top-level heartbeat prints every `heartbeat_sec` to `run.log`.
- Each in-flight replicate writes `outputs/.../reps/rep####_partial.json` and updates it during sub-steps (event start, catalog chunk progress).
- Each completed replicate writes `outputs/.../reps/rep####.json`.

Copied snapshots:
- `final_final_final/artifacts/run.log` (includes heartbeats and rep completion lines)

## Performance changes that made this practical (no accuracy change)

The catalog injection suite was initially too slow/opaque for comfortable scale-up. The following changes were made to make it **fast and trackable** while preserving exactness:

1. **Chunk-level progress + partial outputs (monitorability)**
   - Per-rep partial JSON written with atomic replace (`.tmp` → rename).
   - Top-level heartbeats summarize progress and forward motion.
   - Commits: `5be2bac` (and earlier monitorability support).

2. **Exact speedup for `distance_mode="spectral_only"` / `"prior_only"`**
   - When the likelihood depends only on redshift (not sky), the catalog sum can be computed by grouping galaxies by unique `z` and using multiplicities.
   - This is mathematically exact; it is not an approximation.
   - A regression test enforces equality with the explicit per-galaxy sum.
   - Commit: `af69b17`
   - Test: `tests/test_dark_sirens_pe_spectral_only_grouping_equivalence.py`

3. **Template preprocessing**
   - Pre-sort template galaxy arrays by redshift once at template load, rather than per-call.
   - Commit: `65f9073`

## Results (GR-truth calibration)

Primary summary:
- `final_final_final/artifacts/summary.json`

Plots:
- `final_final_final/fig_delta_lpd_total_hist.png`
- `final_final_final/fig_delta_lpd_components_hist.png`

Tabular sample dump (one row per replicate):
- `final_final_final/artifacts/delta_lpd_samples.csv`

### Headline numbers (n=512 reps; GR truth)

`ΔLPD_total`:
- mean = **-0.839**
- sd = **0.240**
- max = **+0.076**
- `P(ΔLPD_total ≥ 0) = 1/512 = 0.195%`
- `P(ΔLPD_total ≥ 3) = 0/512`

Component means:
- `ΔLPD_total_data` mean = **-1.374**
- `ΔLPD_total_sel` mean = **+0.534**
- (net is negative: the selection term partially offsets, but does not reverse, the data preference under GR truth)

Runtime:
- elapsed ≈ **3317 s** (~55.3 min) for 512 reps on 256 processes.

### Interpretation of the calibration result

Under a GR-truth synthetic universe generated by this pipeline’s own assumptions:
- The μ-propagation model does **not** produce large positive scores by accident.
- The observed real-data `ΔLPD_total ≈ +3.03` is far outside the GR-truth injection distribution we generated (max `+0.076` in 512 reps).

This materially reduces the probability that the +3 score is a **generic numerical artifact** (e.g. PE prior removal bug, weight underflow/overflow, selection α bookkeeping error) that would also appear under GR truth.

What it does **not** prove:
- It does not prove “new physics”. This injection is a **model-consistency** test. If real data contain systematics not present in the injection generator (catalog completeness mis-modeling, selection-function mismatch, PE systematics, etc.), real data can still land in the positive tail for non-physical reasons.

## What to do next (to decide “physics vs systematics”)

The strongest next steps are:

1. **Non-GR truth injections (`truth_model=mu`)**
   - Confirms the tool has *power* (recovers positive `ΔLPD_total` when μ-propagation is true).
   - Quantifies how many events / what SNR distribution is needed to reach `ΔLPD≈3` under a realistic μ truth.

2. **Stress injections that deliberately break assumptions**
   - Inject GR truth but perturb the catalog/selection model in controlled ways (e.g. wrong `f_miss(z)`, magnitude-limit drift proxies, altered `n(z)` templates).
   - This answers: “can plausible catalog/selection mismodeling generate +3 without new physics?”

3. **Event-level “where does it come from?” comparisons**
   - Use the existing jackknife tables from the real-data production runs to confirm whether the same hero-event pattern appears (or vanishes) under injections.

4. **Tighten null batteries around the selection normalization**
   - The real-data mechanism looks selection/distance-distribution dominated. Continue targeted nulls that isolate α vs data terms and vary selection assumptions in a controlled sweep.

## Files copied into this bundle

- Figures:
  - `final_final_final/fig_delta_lpd_total_hist.png`
  - `final_final_final/fig_delta_lpd_components_hist.png`
- Artifacts:
  - `final_final_final/artifacts/summary.json`
  - `final_final_final/artifacts/delta_lpd_samples.csv`
  - `final_final_final/artifacts/run.log`
  - `final_final_final/artifacts/job.sh`
  - `final_final_final/artifacts/manifest.json`
  - `final_final_final/artifacts/missing_pre_meta.json`
  - `final_final_final/artifacts/selection_alpha.npz`

